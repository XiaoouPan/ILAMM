# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' The function fits (high-dimensional) regularized regression with non-convex penalties: Lasso, SCAD and MCP, and it's implemented via I-LAMM algorithm.
#'
#' The observed data are \eqn{(Y, X)}, where \eqn{Y} is an \eqn{n}-dimensional response vector and \eqn{X} is an \eqn{n} by \eqn{d} design matrix. We assume that \eqn{Y} depends on \eqn{X} through a linear model \eqn{Y = X \beta + \epsilon}, where \eqn{\epsilon} is an \eqn{n}-dimensional noise vector whose distribution can be asymmetrix and/or heavy-tailed. The design matrix \eqn{X} can be either high-dimensional or low-dimensional. Tunning parameter \eqn{\lambda} has a default setting but it can be user-specified. All the arguments except for \eqn{X} and \eqn{Y} have default settings.
#'
#' @title Non-convex regularized regression
#' @param X An \eqn{n} by \eqn{d} design matrix with each row being a sample and each column being a variable, either low-dimensional data (\eqn{d \le n}) or high-dimensional data (\eqn{d > n}) are allowed.
#' @param Y A continuous response vector with length \eqn{n}.
#' @param lambda Tuning parameter of regularized regression, its specified value should be positive. The default value is determined in this way: define \eqn{\lambda_max = max(|Y^T X|) / n}, and \eqn{\lambda_min = 0.01 * \lambda_max}, then \eqn{\lambda = exp(0.7 * log(\lambda_max) + 0.3 * log(\lambda_min))}.
#' @param penalty Type of non-convex penalties with default setting "SCAD", possible choices are: "Lasso", "SCAD" and "MCP".
#' @param phi0 The initial value of the isotropic parameter \eqn{\phi} in I-LAMM algorithm. The defalut value is 0.001.
#' @param gamma The inflation parameter in I-LAMM algorithm, in each iteration of I-LAMM, we will inflate \eqn{\phi} by \eqn{\gamma}. The defalut value is 1.5.
#' @param epsilon_c The tolerance level for contraction stage, iteration of contraction will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_c}. The defalut value is 1e-4.
#' @param epsilon_t The tolerance level for tightening stage, iteration of tightening will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_t}. The defalut value is 1e-4.
#' @param iteMax The maximal number of iteration in either contraction or tightening stage, if this number is reached, the convergence of I-LAMM is failed. The defalut value is 500.
#' @param intercept Boolean value indicating whether an intercept term should be included into the model. The default setting is \code{FALSE}.
#' @param itcpIncluded Boolean value indicating whether a column of 1's has been included in the design matrix \eqn{X}. The default setting is \code{FALSE}.
#' @return A list including the following terms will be returned:
#' \itemize{
#' \item \code{beta} The estimated \eqn{\beta}, a vector with length d + 1, with the first one being the value of intercept (0 if \code{intercept = FALSE}).
#' \item \code{phi} The final value of the isotropic parameter \eqn{\phi} in the last iteration of I-LAMM algorithm.
#' \item \code{penalty} The type of penalty.
#' \item \code{lambda} The value of \eqn{\lambda}.
#' \item \code{IteTightening} The number of tightenings in I-LAMM algorithm, and it's 0 if \code{penalty = "Lasso"}.
#' }
#' @author Xiaoou Pan, Qiang Sun, Wen-Xin Zhou
#' @references Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist. 46 814–841.
#' @seealso \code{\link{cvNcvxReg}}
#' @examples
#' n = 50
#' d = 100
#' set.seed(2018)
#' X = matrix(rnorm(n * d), n, d)
#' beta = c(rep(2, 3), rep(0, d - 3))
#' Y = X %*% beta + rnorm(n)
#' # Fit SCAD without intercept
#' fit = ncvxReg(X, Y)
#' fit$beta
#' # Fit MCP with intercept
#' fit = ncvxReg(X, Y, penalty = "MCP", intercept = TRUE)
#' fit$beta
#' @export
ncvxReg <- function(X, Y, lambda = -1, penalty = "SCAD", phi0 = 0.001, gamma = 1.5, epsilon_c = 0.001, epsilon_t = 0.001, iteMax = 500L, intercept = FALSE, itcpIncluded = FALSE) {
    .Call('_ILAMM_ncvxReg', PACKAGE = 'ILAMM', X, Y, lambda, penalty, phi0, gamma, epsilon_c, epsilon_t, iteMax, intercept, itcpIncluded)
}

#' The function fits (high-dimensional) Huber regularized regression with non-convex penalties: Lasso, SCAD and MCP, and it's implemented via I-LAMM algorithm.
#'
#' The observed data are \eqn{(Y, X)}, where \eqn{Y} is an \eqn{n}-dimensional response vector and \eqn{X} is an \eqn{n} by \eqn{d} design matrix. We assume that \eqn{Y} depends on \eqn{X} through a linear model \eqn{Y = X \beta + \epsilon}, where \eqn{\epsilon} is an \eqn{n}-dimensional noise vector whose distribution can be asymmetrix and/or heavy-tailed. The design matrix \eqn{X} can be either high-dimensional or low-dimensional. Tunning parameters \eqn{\lambda} and \eqn{\tau} have default settings but they can be user-specified. All the arguments except for \eqn{X} and \eqn{Y} have default settings.
#'
#' @title Non-convex regularized Huber regression
#' @param X An \eqn{n} by \eqn{d} design matrix with each row being a sample and each column being a variable, either low-dimensional data (\eqn{d \le n}) or high-dimensional data (\eqn{d > n}) are allowed.
#' @param Y A continuous response vector with length \eqn{n}.
#' @param lambda Tuning parameter of regularized regression, its specified value should be positive. The default value is determined in this way: define \eqn{\lambda_max = max(|Y^T X|) / n}, and \eqn{\lambda_min = 0.01 * \lambda_max}, then \eqn{\lambda = exp(0.7 * log(\lambda_max) + 0.3 * log(\lambda_min))}.
#' @param penalty Type of non-convex penalties with default setting "SCAD", possible choices are: "Lasso", "SCAD" and "MCP".
#' @param tau Robustness parameter of Huber loss function, its specified value should be positive. The default value is determined in this way: define \eqn{R} as the residual from Lasso by fitting \code{ncvxReg} with \code{lambda}, and \eqn{\sigma_MAD = median(|R - median(R)|) / \Phi^(-1)(3/4)} is the median absolute deviation estimator, then \eqn{\tau = \sigma_MAD \sqrt(n / log(nd))}.
#' @param phi0 The initial value of the isotropic parameter \eqn{\phi} in I-LAMM algorithm. The defalut value is 0.001.
#' @param gamma The inflation parameter in I-LAMM algorithm, in each iteration of I-LAMM, we will inflate \eqn{\phi} by \eqn{\gamma}. The defalut value is 1.5.
#' @param epsilon_c The tolerance level for contraction stage, iteration of contraction will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_c}. The defalut value is 1e-4.
#' @param epsilon_t The tolerance level for tightening stage, iteration of tightening will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_t}. The defalut value is 1e-4.
#' @param iteMax The maximal number of iteration in either contraction or tightening stage, if this number is reached, the convergence of I-LAMM is failed. The defalut value is 500.
#' @param intercept Boolean value indicating whether an intercept term should be included into the model. The default setting is \code{FALSE}.
#' @param itcpIncluded Boolean value indicating whether a column of 1's has been included in the design matrix \eqn{X}. The default setting is \code{FALSE}.
#' @param tf Boolean value indicating whether a tuning-free principle is applied to calibrate the value of \code{tau}. The default setting is \code{FALSE}.
#' @param constTau The constant used in tuning-free procedure to update \code{tau}. The default value is 2.
#' @return A list including the following terms will be returned:
#' \itemize{
#' \item \code{beta} The estimated \eqn{\beta}, a vector with length d + 1, with the first one being the value of intercept (0 if \code{intercept = FALSE}).
#' \item \code{phi} The final value of the isotropic parameter \eqn{\phi} in the last iteration of I-LAMM algorithm.
#' \item \code{penalty} The type of penalty.
#' \item \code{lambda} The value of \eqn{\lambda}.
#' \item \code{tau} The value of \eqn{\tau}.
#' \item \code{IteTightening} The number of tightenings in I-LAMM algorithm, and it's 0 if \code{penalty = "Lasso"}.
#' }
#' @author Xiaoou Pan, Qiang Sun, Wen-Xin Zhou
#' @references Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist. 46 814–841.
#' @references Wang, L., Zheng, C., Zhou, W. and Zhou, W.-X. (2018). A New Principle for Tuning-Free Huber Regression. Preprint.
#' @seealso \code{\link{cvNcvxHuberReg}}
#' @examples
#' n = 50
#' d = 100
#' set.seed(2018)
#' X = matrix(rnorm(n * d), n, d)
#' beta = c(rep(2, 3), rep(0, d - 3))
#' Y = X %*% beta + rlnorm(n, 0, 1.2) - exp(1.2^2 / 2)
#' # Fit Huber-SCAD without intercept
#' fit = ncvxHuberReg(X, Y)
#' fit$beta
#' # Fit Huber-MCP with intercept
#' fit = ncvxHuberReg(X, Y, penalty = "MCP", intercept = TRUE)
#' fit$beta
#' @export
ncvxHuberReg <- function(X, Y, lambda = -1, penalty = "SCAD", tau = -1, phi0 = 0.001, gamma = 1.5, epsilon_c = 0.001, epsilon_t = 0.001, iteMax = 500L, intercept = FALSE, itcpIncluded = FALSE, tf = FALSE, constTau = 2) {
    .Call('_ILAMM_ncvxHuberReg', PACKAGE = 'ILAMM', X, Y, lambda, penalty, tau, phi0, gamma, epsilon_c, epsilon_t, iteMax, intercept, itcpIncluded, tf, constTau)
}

#' The function performs k-fold cross validation for (high-dimensional) regularized regression with non-convex penalties: Lasso, SCAD and MCP, and it's implemented via I-LAMM algorithm.
#'
#' The observed data are \eqn{(Y, X)}, where \eqn{Y} is an \eqn{n}-dimensional response vector and \eqn{X} is an \eqn{n} by \eqn{d} design matrix. We assume that \eqn{Y} depends on \eqn{X} through a linear model \eqn{Y = X \beta + \epsilon}, where \eqn{\epsilon} is an \eqn{n}-dimensional noise vector whose distribution can be asymmetrix and/or heavy-tailed. The design matrix \eqn{X} can be either high-dimensional or low-dimensional. The sequence of \eqn{\lambda}'s has a default setting but it can be user-specified. All the arguments except for \eqn{X} and \eqn{Y} have default settings.
#'
#' @title K-fold cross validation for non-convex regularized regression
#' @param X An \eqn{n} by \eqn{d} design matrix with each row being a sample and each column being a variable, either low-dimensional data (\eqn{d \le n}) or high-dimensional data (\eqn{d > n}) are allowed.
#' @param Y A continuous response vector with length \eqn{n}.
#' @param lSeq Sequence of tuning parameter of regularized regression \eqn{\lambda}, every element should be positive. If it's not specified, the default sequence is generated in this way: define \eqn{\lambda_max = max(|Y^T X|) / n}, and \eqn{\lambda_min = 0.01 * \lambda_max}, then \code{lseq} is a sequence from \eqn{\lambda_max} to \eqn{\lambda_min} that decreases uniformly on log scale.
#' @param nlambda Number of \eqn{\lambda} to generate the default sequence \code{lSeq}. It's not necessary if \code{lSeq} is specified. The default value is 30.
#' @param penalty Type of non-convex penalties with default setting "SCAD", possible choices are: "Lasso", "SCAD" and "MCP".
#' @param phi0 The initial value of the isotropic parameter \eqn{\phi} in I-LAMM algorithm. The defalut value is 0.001.
#' @param gamma The inflation parameter in I-LAMM algorithm, in each iteration of I-LAMM, we will inflate \eqn{\phi} by \eqn{\gamma}. The defalut value is 1.5.
#' @param epsilon_c The tolerance level for contraction stage, iteration of contraction will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_c}. The defalut value is 1e-4.
#' @param epsilon_t The tolerance level for tightening stage, iteration of tightening will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_t}. The defalut value is 1e-4.
#' @param iteMax The maximal number of iteration in either contraction or tightening stage, if this number is reached, the convergence of I-LAMM is failed. The defalut value is 500.
#' @param nfolds The number of folds to conduct cross validation, values that are greater than 10 are not recommended, and it'll be modified to 10 if the input is greater than 10. The default value is 3.
#' @param intercept Boolean value indicating whether an intercept term should be included into the model. The default setting is \code{FALSE}.
#' @param itcpIncluded Boolean value indicating whether a column of 1's has been included in the design matrix \eqn{X}. The default setting is \code{FALSE}.
#' @return A list including the following terms will be returned:
#' \itemize{
#' \item \code{beta} The estimated \eqn{\beta} with \eqn{\lambda} determined by cross validation, it's a vector with length d + 1, with the first one being the value of intercept (0 if \code{intercept = FALSE}).
#' \item \code{penalty} The type of penalty.
#' \item \code{lambdaSeq} The sequence of \eqn{\lambda}'s for cross validation.
#' \item \code{mse} The mean squared error from cross validation, it's a vector with length \code{nlambda}.
#' \item \code{lambdaMin} The value of \eqn{\lambda} in \code{lambdaSeq} that minimized \code{mse}.
#' \item \code{nfolds} The number of folds for cross validation.
#' }
#' @author Xiaoou Pan, Qiang Sun, Wen-Xin Zhou
#' @references Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist. 46 814–841.
#' @seealso \code{\link{ncvxReg}}
#' @examples
#' n = 50
#' d = 100
#' set.seed(2018)
#' X = matrix(rnorm(n * d), n, d)
#' beta = c(rep(2, 3), rep(0, d - 3))
#' Y = X %*% beta + rnorm(n)
#' # Fit SCAD without intercept, with lambda determined by 3-folds cross validation
#' fit = cvNcvxReg(X, Y)
#' fit$beta
#' fit$lambdaMin
#' # Fit MCP with intercept, with lambda determined by 5-folds cross validation
#' fit = cvNcvxReg(X, Y, penalty = "MCP", intercept = TRUE, nfolds = 5)
#' fit$beta
#' fit$lambdaMin
#' @export
cvNcvxReg <- function(X, Y, lSeq = NULL, nlambda = 30L, penalty = "SCAD", phi0 = 0.001, gamma = 1.5, epsilon_c = 0.001, epsilon_t = 0.001, iteMax = 500L, nfolds = 3L, intercept = FALSE, itcpIncluded = FALSE) {
    .Call('_ILAMM_cvNcvxReg', PACKAGE = 'ILAMM', X, Y, lSeq, nlambda, penalty, phi0, gamma, epsilon_c, epsilon_t, iteMax, nfolds, intercept, itcpIncluded)
}

#' The function performs k-fold cross validation for (high-dimensional) Huber regularized regression with non-convex penalties: Lasso, SCAD and MCP, and it's implemented via I-LAMM algorithm.
#'
#' The observed data are \eqn{(Y, X)}, where \eqn{Y} is an \eqn{n}-dimensional response vector and \eqn{X} is an \eqn{n} by \eqn{d} design matrix. We assume that \eqn{Y} depends on \eqn{X} through a linear model \eqn{Y = X \beta + \epsilon}, where \eqn{\epsilon} is an \eqn{n}-dimensional noise vector whose distribution can be asymmetrix and/or heavy-tailed. The design matrix \eqn{X} can be either high-dimensional or low-dimensional. The sequence of \eqn{\lambda}'s and \eqn{\tau}'s have default settings but they can be user-specified. All the arguments except for \eqn{X} and \eqn{Y} have default settings.
#'
#' @title K-fold cross validation for non-convex regularized Huber regression
#' @param X An \eqn{n} by \eqn{d} design matrix with each row being a sample and each column being a variable, either low-dimensional data (\eqn{d \le n}) or high-dimensional data (\eqn{d > n}) are allowed.
#' @param Y A continuous response vector with length \eqn{n}.
#' @param lSeq Sequence of tuning parameter of regularized regression \eqn{\lambda}, every element should be positive. If it's not specified, the default sequence is generated in this way: define \eqn{\lambda_max = max(|Y^T X|) / n}, and \eqn{\lambda_min = 0.01 * \lambda_max}, then \code{lseq} is a sequence from \eqn{\lambda_max} to \eqn{\lambda_min} that decreases uniformly on log scale.
#' @param nlambda Number of \eqn{\lambda} to generate the default sequence \code{lSeq}. It's not necessary if \code{lSeq} is specified. The default value is 30.
#' @param penalty Type of non-convex penalties with default setting "SCAD", possible choices are: "Lasso", "SCAD" and "MCP".
#' @param tSeq Sequence of robustness parameter of Huber loss \eqn{\tau}, every element should be positive. If it's not specified, the default sequence is generated in this way: define \eqn{R} as the residual from Lasso by fitting \code{cvNcvxReg} with \code{lSeq}, and \eqn{\sigma_MAD = median(|R - median(R)|) / \Phi^(-1)(3/4)} is the median absolute deviation estimator, then \code{tSeq} = \eqn{2^j * \sigma_MAD \sqrt(n / log(nd))}, where \eqn{j} are integers from -\code{ntau}/2 to \code{ntau}/2.
#' @param ntau Number of \eqn{\tau} to generate the default sequence \code{tSeq}. It's not necessary if \code{tSeq} is specified. The default value is 5.
#' @param phi0 The initial value of the isotropic parameter \eqn{\phi} in I-LAMM algorithm. The defalut value is 0.001.
#' @param gamma The inflation parameter in I-LAMM algorithm, in each iteration of I-LAMM, we will inflate \eqn{\phi} by \eqn{\gamma}. The defalut value is 1.5.
#' @param epsilon_c The tolerance level for contraction stage, iteration of contraction will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_c}. The defalut value is 1e-4.
#' @param epsilon_t The tolerance level for tightening stage, iteration of tightening will stop when \eqn{||\beta_new - \beta_old||_2 / \sqrt(d + 1) < \epsilon_t}. The defalut value is 1e-4.
#' @param iteMax The maximal number of iteration in either contraction or tightening stage, if this number is reached, the convergence of I-LAMM is failed. The defalut value is 500.
#' @param nfolds The number of folds to conduct cross validation, values that are greater than 10 are not recommended, and it'll be modified to 10 if the input is greater than 10. The default value is 3.
#' @param intercept Boolean value indicating whether an intercept term should be included into the model. The default setting is \code{FALSE}.
#' @param itcpIncluded Boolean value indicating whether a column of 1's has been included in the design matrix \eqn{X}. The default setting is \code{FALSE}.
#' @param tf Boolean value indicating whether a tuning-free principle is applied to calibrate the value of \code{tau}. The default setting is \code{FALSE}.
#' @param constTau The constant used in tuning-free procedure to update \code{tau}. The default value is 2.
#' @return A list including the following terms will be returned:
#' \itemize{
#' \item \code{beta} The estimated \eqn{\beta} with \eqn{\lambda} and \eqn{\tau} determined by cross validation, it's a vector with length d + 1, with the first one being the value of intercept (0 if \code{intercept = FALSE}).
#' \item \code{penalty} The type of penalty.
#' \item \code{lambdaSeq} The sequence of \eqn{\lambda}'s for cross validation.
#' \item \code{tauSeq} The sequence of \eqn{\tau}'s for cross validation.
#' \item \code{mse} The mean squared error from cross validation, it's a matrix with dimension \code{nlambda} by \code{ntau}.
#' \item \code{lambdaMin} The value of \eqn{\lambda} in \code{lSeq} that minimized \code{mse}.
#' \item \code{tauMin} The value of \eqn{\tau} in \code{tSeq} that minimized \code{mse}.
#' \item \code{nfolds} The number of folds for cross validation.
#' }
#' @author Xiaoou Pan, Qiang Sun, Wen-Xin Zhou
#' @references Fan, J., Liu, H., Sun, Q. and Zhang, T. (2018). I-LAMM for sparse learning: Simultaneous control of algorithmic complexity and statistical error. Ann. Statist. 46 814–841.
#' @references Wang, L., Zheng, C., Zhou, W. and Zhou, W.-X. (2018). A New Principle for Tuning-Free Huber Regression. Preprint.
#' @seealso \code{\link{ncvxHuberReg}}
#' @examples
#' n = 50
#' d = 100
#' set.seed(2018)
#' X = matrix(rnorm(n * d), n, d)
#' beta = c(rep(2, 3), rep(0, d - 3))
#' Y = X %*% beta + rlnorm(n, 0, 1.2) - exp(1.2^2 / 2)
#' # Fit SCAD without intercept, with lambda and tau determined by 3-folds cross validation
#' fit = cvNcvxHuberReg(X, Y)
#' fit$beta
#' fit$lambdaMin
#' fit$tauMin
#' # Fit MCP with intercept, with lambda and tau determined by 3-folds cross validation
#' fit = cvNcvxHuberReg(X, Y, penalty = "MCP", intercept = TRUE)
#' fit$beta
#' fit$lambdaMin
#' fit$tauMin
#' @export
cvNcvxHuberReg <- function(X, Y, lSeq = NULL, nlambda = 30L, penalty = "SCAD", tSeq = NULL, ntau = 5L, phi0 = 0.001, gamma = 1.5, epsilon_c = 0.001, epsilon_t = 0.001, iteMax = 500L, nfolds = 3L, intercept = FALSE, itcpIncluded = FALSE, tf = FALSE, constTau = 2) {
    .Call('_ILAMM_cvNcvxHuberReg', PACKAGE = 'ILAMM', X, Y, lSeq, nlambda, penalty, tSeq, ntau, phi0, gamma, epsilon_c, epsilon_t, iteMax, nfolds, intercept, itcpIncluded, tf, constTau)
}

